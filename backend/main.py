from fastapi import FastAPI, UploadFile, File
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
load_dotenv()  # Loads from .env file
import supabase
from supabase import create_client, Client  # <-- Make sure this is imported
import os
import io
import requests  # <-- ADD THIS
import json      # <-- ADD THIS

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "DocTalk AI Backend is Live!"}

load_dotenv()  # Loads from .env file

# Ollama configuration - ADD THIS
OLLAMA_URL = "http://localhost:11434"

# supabase client
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

@app.get("/test-db")
def test_db():
    try:
        response = supabase.table('documents').select("*").limit(1).execute()
        return {"status": "Database connected!", "data": response.data}
    except Exception as e:
        return {"error": str(e)}


# embedding func
def get_embedding(text: str) -> list:
    """Get embedding vector from Ollama"""
    payload = {
        "model": "nomic-embed-text", 
        "prompt": text
    }
    response = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload)
    response.raise_for_status()
    return response.json()["embedding"]

@app.post("/upload")
async def upload_pdf(file: UploadFile = File(...)):
    print(f"üì• Received file: {file.filename}")
    
    # 1. Read PDF
    contents = await file.read()
    
    # 2. Extract text with pypdf
    pdf_text = ""
    reader = PdfReader(io.BytesIO(contents))
    for page in reader.pages:
        pdf_text += page.extract_text() + "\n"
    
    print(f"üìÑ Extracted {len(pdf_text)} characters of text")
    
    # 3. Chunk text with LangChain
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_text(pdf_text)
    
    print(f"‚úÇÔ∏è Split into {len(chunks)} chunks")
    
    # 4. Store document in Supabase
    print("üíæ Storing document in database...")
    document_data = {
        "file_name": file.filename
    }
    document_response = supabase.table("documents").insert(document_data).execute()
    document_id = document_response.data[0]["id"]
    print(f"   Document stored with ID: {document_id}")
    
    # 5. Generate embeddings and store chunks
    print("üß† Generating embeddings and storing chunks...")
    stored_chunks = 0
    for i, chunk in enumerate(chunks):
        embedding = get_embedding(chunk)
        
        chunk_data = {
            "document_id": document_id,
            "chunk_text": chunk,
            "embedding": embedding
        }
        supabase.table("document_chunks").insert(chunk_data).execute()
        stored_chunks += 1
        
        # Print progress for first few chunks
        if i < 2:
            print(f"   Chunk {i+1}: {len(embedding)} dimensions - {embedding[:3]}...")
    
    print(f"üíæ Successfully stored {stored_chunks} chunks in database")
    
    return {
        "filename": file.filename,
        "text_length": len(pdf_text),
        "chunk_count": len(chunks),
        "document_id": document_id,
        "stored_chunks": stored_chunks,
        "message": "PDF successfully processed and stored in database!"
    }

@app.post("/chat")
async def chat_with_document(question: str, document_id: str):
    print(f"üí¨ Chat question: {question}")
    print(f"üìÑ Querying document: {document_id}")
    
    # 1. Embed the question
    question_embedding = get_embedding(question)
    print(f"üß† Question embedded with {len(question_embedding)} dimensions")
    
    # 2. Find similar chunks using vector search
    print("üîç Searching for relevant chunks...")
    response = supabase.rpc(
        'match_chunks',
        {
            'query_embedding': question_embedding,
            'match_count': 5,
            'filter_document_id': document_id  # Changed from 'document_id'
        }
    ).execute()
    
    relevant_chunks = response.data
    print(f"üìö Found {len(relevant_chunks)} relevant chunks")
    
    # 3. Build context from relevant chunks
    context = "\n\n".join([chunk['chunk_text'] for chunk in relevant_chunks])
    
    # 4. Generate AI response
    print("ü§ñ Generating AI response...")
    answer = generate_answer(question, context)
    if len(answer) > 0:
        print(f" Answer received: {answer}")
    else:
        print(f" Error: couldn't get answer")



    return {
        "question": question,
        "document_id": document_id,
        "relevant_chunks_count": len(relevant_chunks),
        "answer": answer
    }

def generate_answer(question: str, context: str) -> str:
    """Generate answer using Ollama chat model"""
    prompt = f"""Based on the following context, answer the user's question.

Context:
{context}

Question: {question}

Answer:"""
    
    payload = {
        "model": "deepseek-r1:14b",
        "prompt": prompt,
        "stream": False
    }
    
    response = requests.post(f"{OLLAMA_URL}/api/generate", json=payload)
    response.raise_for_status()
    return response.json()["response"]

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)